{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc04d48",
   "metadata": {},
   "source": [
    "# Social Media ETL & Analytics Pipeline\n",
    "**Objective:** Fetch and analyze posts from Twitter and YouTube, normalize data, compute engagement metrics, and identify top posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3be6ce",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "- `requests` for API requests\n",
    "- `pandas` for data handling\n",
    "- `logging` to track pipeline execution and errors\n",
    "- `os` and `datetime` for file paths and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621ccc97",
   "metadata": {},
   "source": [
    "## API Keys\n",
    "Set Twitter Bearer Token and YouTube API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load API keys from environment variables\n",
    "TWITTER_BEARER_TOKEN = os.getenv(\"TWITTER_BEARER_TOKEN\", \"your-twitter-token-here\")\n",
    "YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\", \"your-youtube-key-here\")\n",
    "\n",
    "# Twitter headers\n",
    "TWITTER_HEADERS = {\"Authorization\": f\"Bearer {TWITTER_BEARER_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d50e5",
   "metadata": {},
   "source": [
    "## Logging\n",
    "Setup a log file to record pipeline execution and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61cf1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using logging to create a log file and store it in the work directory \n",
    "LOG_FOLDER = r\"D:\\Data_Engineer_SE\\logs\"\n",
    "os.makedirs(LOG_FOLDER, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(LOG_FOLDER, 'social_data_pipeline.log'),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd855a36",
   "metadata": {},
   "source": [
    "## Data Storage\n",
    "Create folder and master CSV file to store all posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab18dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv file to store the data fetched \n",
    "DATA_FOLDER = r\"D:\\Data_Engineer_SE\"\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "MASTER_FILE = os.path.join(DATA_FOLDER, \"social_data_master.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fcf5f8",
   "metadata": {},
   "source": [
    "## Fetch Functions\n",
    "- `fetch_twitter_posts()` retrieves tweets\n",
    "- `fetch_youtube_posts()` retrieves videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_twitter_posts(query=\"Data warehouse\", max_results=10):\n",
    "    \"\"\"Fetch recent tweets from Twitter API.\"\"\"\n",
    "    url = f\"https://api.twitter.com/2/tweets/search/recent?query={query}&max_results={max_results}&tweet.fields=public_metrics,created_at,author_id\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=TWITTER_HEADERS) #get data from twitter using the api\n",
    "        response.raise_for_status()\n",
    "        tweets = response.json().get(\"data\", []) # store the data returned in tweets for using later \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Twitter API request failed: {e}\") # raising error if the extraction failed\n",
    "        return []\n",
    "\n",
    "    records = []\n",
    "    for t in tweets:   # iterate the java script file returnd to identify the columns required and assign keys for them such as: post_id ,platform :Twitter , and author_id .....\n",
    "        records.append({\n",
    "            \"post_id\": str(t.get(\"id\", \"\")),\n",
    "            \"platform\": \"Twitter\",\n",
    "            \"author_id\": str(t.get(\"author_id\", \"\")),\n",
    "            \"content\": str(t.get(\"text\", \"\")),\n",
    "            \"likes\": t.get(\"public_metrics\", {}).get(\"like_count\", 0),\n",
    "            \"comments\": t.get(\"public_metrics\", {}).get(\"reply_count\", 0),\n",
    "            \"shares\": t.get(\"public_metrics\", {}).get(\"retweet_count\", 0),\n",
    "            \"post_date\": pd.to_datetime(t.get(\"created_at\", \"\"), errors='coerce')\n",
    "        })\n",
    "    logging.info(f\"Fetched {len(records)} tweets\") #record the process in the log file and show the total rows fetched \n",
    "    return records\n",
    "\n",
    "\n",
    "def fetch_youtube_posts(query=\"Data warehouse \", max_results=10):\n",
    "    \"\"\"Fetch recent YouTube videos.\"\"\"\n",
    "    search_url = f\"https://www.googleapis.com/youtube/v3/search?part=snippet&q={query}&type=video&maxResults={max_results}&key={YOUTUBE_API_KEY}\"\n",
    "    try:\n",
    "        response = requests.get(search_url).json()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"YouTube search request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "    records = []\n",
    "    for item in response.get(\"items\", []):\n",
    "        video_id = item[\"id\"].get(\"videoId\")\n",
    "        if not video_id:\n",
    "            continue\n",
    "        snippet = item[\"snippet\"]\n",
    "        stats_url = f\"https://www.googleapis.com/youtube/v3/videos?part=statistics&id={video_id}&key={YOUTUBE_API_KEY}\"\n",
    "        try:\n",
    "            stats_response = requests.get(stats_url).json()\n",
    "            stats_items = stats_response.get(\"items\", [])\n",
    "            if not stats_items:\n",
    "                continue\n",
    "            stats = stats_items[0].get(\"statistics\", {})\n",
    "        except Exception as e:\n",
    "            logging.error(f\"YouTube stats request failed for video {video_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        records.append({\n",
    "            \"post_id\": video_id,\n",
    "            \"platform\": \"YouTube\",\n",
    "            \"author_id\": str(snippet.get(\"channelId\", \"\")),\n",
    "            \"content\": str(snippet.get(\"title\", \"\")),\n",
    "            \"likes\": int(stats.get(\"likeCount\", 0)),\n",
    "            \"comments\": int(stats.get(\"commentCount\", 0)),\n",
    "            \"shares\": 0,\n",
    "            \"post_date\": pd.to_datetime(snippet.get(\"publishedAt\", \"\"), errors='coerce')\n",
    "        })\n",
    "    logging.info(f\"Fetched {len(records)} YouTube videos\")\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beddc75e",
   "metadata": {},
   "source": [
    "## Save Data\n",
    "- Save all posts to a single master CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43397c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, master_file=MASTER_FILE):\n",
    "    \"\"\"Append new records into master CSV file without overwriting existing ones.\"\"\"\n",
    "    if os.path.exists(master_file):\n",
    "        try:\n",
    "            # Load existing data\n",
    "            df_master = pd.read_csv(master_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "            # Concatenate old + new\n",
    "            df_combined = pd.concat([df_master, df], ignore_index=True)\n",
    "\n",
    "            # Drop duplicate posts by post_id + platform (to avoid duplicates)\n",
    "            df_combined.drop_duplicates(subset=[\"post_id\", \"platform\"], keep=\"last\", inplace=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to read master file: {e}\") # error if can not find the file beacue of not existence or premission denied \n",
    "            df_combined = df.copy()\n",
    "    else:\n",
    "        df_combined = df.copy() # if there is not data copy the fetched only\n",
    "\n",
    "    # Ensure string encoding\n",
    "    df_combined['content'] = df_combined['content'].fillna(\"\").astype(str) #treate the content as string because it may contain spaces or emojies\n",
    "\n",
    "    try:\n",
    "        # Save back to master file\n",
    "        df_combined.to_csv(master_file, index=False, encoding=\"utf-8-sig\") # use standerd encoding because of arabic or any language result\n",
    "        logging.info(f\"Master CSV updated: {master_file}, total rows: {len(df_combined)}\") # update the log and return the length of data \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save master CSV: {e}\") #error if can not save the data because of permissions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d844000",
   "metadata": {},
   "source": [
    "## ETL Pipeline\n",
    "- Fetch, normalize, compute engagement score, and save to master CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_process_posts(query=\"Data warehouse\", max_results=10):\n",
    "    \"\"\"Run ETL pipeline.\"\"\"\n",
    "    try:\n",
    "        twitter_data = fetch_twitter_posts(query=query, max_results=max_results) #fetch data from twitter using the (data warehouse )query and max results per run query\n",
    "        youtube_data = fetch_youtube_posts(query=query, max_results=max_results) #fetch data from youtube using the (data warehouse) query and max results also \n",
    "        all_data = twitter_data + youtube_data #collect the data returned into on place\n",
    "\n",
    "        df = pd.DataFrame(all_data) # make unified dataframe of data\n",
    "\n",
    "        # Transform & normalize\n",
    "        df['post_id'] = df['post_id'].astype(str) # make all the posts ids as a string so that we can handel it twitter is numeric but you tube is string for ids\n",
    "        df['author_id'] = df['author_id'].astype(str) # make all the posts authors id as a string so that we can handel it twitter is numeric but you tube is string for ids\n",
    "        df['post_date'] = pd.to_datetime(df['post_date'], errors='coerce') # convert type of post date to date and coerce used to make it as nat if could not convert it\n",
    "        df['likes'] = df['likes'].fillna(0).astype(int) # likes as integer and fill null values with 0 \n",
    "        df['comments'] = df['comments'].fillna(0).astype(int) # comments as integer and fill null values with 0\n",
    "        df['shares'] = df['shares'].fillna(0).astype(int) # shares as integer and fill null values with 0\n",
    "        df['content'] = df['content'].fillna(\"\").astype(str) # content as string and fill null values with empty string\n",
    "        df['platform'] = df['platform'].str.title() # platform into title to capitalize first charachter\n",
    "        df['engagement_score'] = df['likes'] + df['comments'] + df['shares'] # sum of likes ,comments ,and shares as engagement score\n",
    "\n",
    "        # Save to master CSV\n",
    "        save_data(df) # call save data function to save the data \n",
    "\n",
    "        logging.info(\"Pipeline executed successfully.\") # store the process in the log as sucessfully\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline execution failed: {e}\") # return error if can not pulled the data\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85e087",
   "metadata": {},
   "source": [
    "## Analytics\n",
    "Compute:\n",
    "- Daily engagement per platform\n",
    "- Top posts overall and per platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_daily_engagement(df):\n",
    "    daily_engagement = (\n",
    "        df.groupby([df['post_date'].dt.date, 'platform']) # we group by date , platform ,and aggregate function for every row\n",
    "          .agg(\n",
    "              daily_posts=('post_id', 'count'), # count posts\n",
    "              daily_likes=('likes', 'sum'), # sum likes\n",
    "              daily_comments=('comments', 'sum'), #sum comments\n",
    "              daily_shares=('shares', 'sum'), # sum shares\n",
    "              daily_engagement=('engagement_score', 'sum') #sum engagement\n",
    "          )\n",
    "          .reset_index() # use the normal index 0.1..2..3 and etc...\n",
    "    )\n",
    "    daily_engagement.rename(columns={'post_date': 'date'}, inplace=True) #rename post_date to date and store the value to the orginal df\n",
    "    return daily_engagement #retuen daily_engagement to use later when we call the function\n",
    "\n",
    "\n",
    "def get_top_posts(df, top_n_overall=5, top_n_per_platform=3):\n",
    "    top_overall = df.sort_values('engagement_score', ascending=False).head(top_n_overall) #sort the data by engagement score and return the most 5 posts for both platforms\n",
    "    top_per_platform = df.groupby('platform', group_keys=False).apply(\n",
    "        lambda x: x.sort_values('engagement_score', ascending=False).head(top_n_per_platform) # choose first 3 using head after sorting per platform\n",
    "    )\n",
    "    return top_overall, top_per_platform #return first 5 of both platforms and first 3 per platform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726e5d4",
   "metadata": {},
   "source": [
    "## Run Pipeline and Analytics\n",
    "- Execute ETL\n",
    "- Compute daily engagement\n",
    "- Show top posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b50d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline run completed. Rows fetched: 20\n"
     ]
    }
   ],
   "source": [
    "# Run the ETL pipeline and fetch all posts from both platforms\n",
    "all_posts_df = fetch_and_process_posts(query=\"Data warehouse\", max_results=10)\n",
    "print(f\"Pipeline run completed. Rows fetched: {len(all_posts_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Engagement per Platform:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>platform</th>\n",
       "      <th>daily_posts</th>\n",
       "      <th>daily_likes</th>\n",
       "      <th>daily_comments</th>\n",
       "      <th>daily_shares</th>\n",
       "      <th>daily_engagement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>Youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>10782</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>11047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>Youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>12242</td>\n",
       "      <td>424</td>\n",
       "      <td>0</td>\n",
       "      <td>12666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-06-04</td>\n",
       "      <td>Youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>7654</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>7735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-04</td>\n",
       "      <td>Youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>2660</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>2691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-26</td>\n",
       "      <td>Youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>16374</td>\n",
       "      <td>524</td>\n",
       "      <td>0</td>\n",
       "      <td>16898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date platform  daily_posts  daily_likes  daily_comments  \\\n",
       "0  2017-06-22  Youtube            1        10782             265   \n",
       "1  2020-02-15  Youtube            1        12242             424   \n",
       "2  2020-06-04  Youtube            1         7654              81   \n",
       "3  2021-06-04  Youtube            1         2660              31   \n",
       "4  2021-10-26  Youtube            1        16374             524   \n",
       "\n",
       "   daily_shares  daily_engagement  \n",
       "0             0             11047  \n",
       "1             0             12666  \n",
       "2             0              7735  \n",
       "3             0              2691  \n",
       "4             0             16898  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute daily engagement per platform using group by platform,date and sum of total engagement per day\n",
    "daily_engagement = compute_daily_engagement(all_posts_df)\n",
    "print(\"Daily Engagement per Platform:\")\n",
    "daily_engagement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f8a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Posts Overall:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esmai\\AppData\\Local\\Temp\\ipykernel_12356\\713350737.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_per_platform = df.groupby('platform', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>content</th>\n",
       "      <th>engagement_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>Database vs Data Warehouse vs Data Lake | What...</td>\n",
       "      <td>24975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>KNOW the difference between Data Base // Data ...</td>\n",
       "      <td>16898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>What is ETL | What is Data Warehouse | OLTP vs...</td>\n",
       "      <td>12666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>Data Warehouse Tutorial For Beginners | Data W...</td>\n",
       "      <td>11047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>SQL Data Warehouse from Scratch | Full Hands-O...</td>\n",
       "      <td>10736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   platform                                            content  \\\n",
       "12  Youtube  Database vs Data Warehouse vs Data Lake | What...   \n",
       "16  Youtube  KNOW the difference between Data Base // Data ...   \n",
       "13  Youtube  What is ETL | What is Data Warehouse | OLTP vs...   \n",
       "17  Youtube  Data Warehouse Tutorial For Beginners | Data W...   \n",
       "19  Youtube  SQL Data Warehouse from Scratch | Full Hands-O...   \n",
       "\n",
       "    engagement_score  \n",
       "12             24975  \n",
       "16             16898  \n",
       "13             12666  \n",
       "17             11047  \n",
       "19             10736  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top 5 posts overall by engagement using group by platform and sum of total engagement_score\n",
    "top_overall, top_per_platform = get_top_posts(all_posts_df)\n",
    "print(\"Top 5 Posts Overall:\")\n",
    "top_overall[['platform', 'content', 'engagement_score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd39ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Posts per Platform:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>content</th>\n",
       "      <th>engagement_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>RT @propchainmaster: Real estate doesn’t lack ...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>RT @propchainmaster: Real estate doesn’t lack ...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>RT @propchainmaster: Real estate doesn’t lack ...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>Database vs Data Warehouse vs Data Lake | What...</td>\n",
       "      <td>24975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>KNOW the difference between Data Base // Data ...</td>\n",
       "      <td>16898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>What is ETL | What is Data Warehouse | OLTP vs...</td>\n",
       "      <td>12666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   platform                                            content  \\\n",
       "3   Twitter  RT @propchainmaster: Real estate doesn’t lack ...   \n",
       "4   Twitter  RT @propchainmaster: Real estate doesn’t lack ...   \n",
       "5   Twitter  RT @propchainmaster: Real estate doesn’t lack ...   \n",
       "12  Youtube  Database vs Data Warehouse vs Data Lake | What...   \n",
       "16  Youtube  KNOW the difference between Data Base // Data ...   \n",
       "13  Youtube  What is ETL | What is Data Warehouse | OLTP vs...   \n",
       "\n",
       "    engagement_score  \n",
       "3                109  \n",
       "4                109  \n",
       "5                109  \n",
       "12             24975  \n",
       "16             16898  \n",
       "13             12666  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top 3 posts per platform using aggregation and group by from the top_per_platform function\n",
    "print(\"Top 3 Posts per Platform:\")\n",
    "top_per_platform[['platform', 'content', 'engagement_score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6ed60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platform\n",
      "Twitter    10\n",
      "Youtube    10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# know how many records pulled\n",
    "print(all_posts_df['platform'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
